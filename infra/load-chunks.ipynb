{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A([Inicio]) --> B[Lectura de documentos]\n",
    "    B --> C[Extracci√≥n de texto]\n",
    "    C --> D[Divisi√≥n en fragmentos relevantes]\n",
    "    D --> E[Vectorizaci√≥n con TF-IDF]\n",
    "    E --> F[Creaci√≥n o actualizaci√≥n de colecci√≥n en Qdrant]\n",
    "    F --> G[Indexaci√≥n de los vectores dispersos]\n",
    "    G --> H([Repositorio de conocimiento listo])\n",
    "\n",
    "    H --> I[Recepci√≥n de consulta del usuario]\n",
    "    I --> J[Vectorizaci√≥n de la consulta con el mismo modelo]\n",
    "    J --> K[B√∫squeda sem√°ntica en Qdrant]\n",
    "    K --> L([Resultados m√°s relevantes])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: pypdf in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (6.1.1)\n",
      "Requirement already satisfied: pandas in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pypdf pandas\n",
    "!pip install -U \"qdrant-client>=1.8.2\"\n",
    "!pip show qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Name :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION = \"code_knowledge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Collection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'code_knowledge' deleted.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False, check_compatibility=False)\n",
    "\n",
    "if client.collection_exists(COLLECTION):\n",
    "    client.delete_collection(COLLECTION)\n",
    "    print(f\"Collection '{COLLECTION}' deleted.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chunk Methods :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks sem√°nticos usando expresiones regulares.\n",
    "    - Respeta l√≠mites de oraci√≥n (., !, ?, ;, :) y saltos de p√°rrafo.\n",
    "    - Mantiene un tama√±o aproximado en palabras.\n",
    "    - Aplica overlap para conservar contexto entre chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text if text else \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = re.split(r\"\\n{2,}\", text)\n",
    "    sentences = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        parts = re.split(r\"(?<=[.!?;:])\\s+\", para)\n",
    "        sentences.extend(parts)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        words = sent.split()\n",
    "        sent_len = len(words)\n",
    "\n",
    "        if current_len + sent_len > chunk_size and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk).strip())\n",
    "\n",
    "            if overlap > 0 and current_chunk:\n",
    "                overlap_words = \" \".join(\" \".join(current_chunk).split()[-overlap:])\n",
    "                current_chunk = [overlap_words] if overlap_words else []\n",
    "                current_len = len(overlap_words.split()) if overlap_words else 0\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_len = 0\n",
    "\n",
    "        current_chunk.append(sent)\n",
    "        current_len += sent_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk).strip())\n",
    "\n",
    "    return [c for c in chunks if c.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus shape: (2145, 4)\n",
      "Upserted batch 1/3 (1024 points)\n",
      "Upserted batch 2/3 (1024 points)\n",
      "Upserted batch 3/3 (97 points)\n",
      "\n",
      "üîé 'principios de c√≥digo limpio y mantenibilidad' ‚Üí top 5\n",
      "\n",
      "‚Ä¢ 0.1543 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p1082 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1082_c2\n",
      "  MacOS Finder window displays the flags as they are saved. The scripts are downloading images from fluentpython.com, which is behind a CDN, so you may see slower results i...\n",
      "\n",
      "‚Ä¢ 0.0852 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p641 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p641_c2\n",
      "  depends on the context! For such purposes as deciding how best to cook a waterfowl once you‚Äôve bagged it, for example, specific observable traits (not all of them‚Äîplumage...\n",
      "\n",
      "‚Ä¢ 0.0811 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p1083 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1083_c1\n",
      "  RU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US 20 flags downloaded in 1.27s $ python3 flags_asyncio.py RU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH ...\n",
      "\n",
      "‚Ä¢ 0.0592 | knowledge/cc_knowledge_book.pdf | p267 | cc_knowledge_book.pdf:p267_c1\n",
      "  236 Chapter 14: Successive ReÔ¨Ånement throw new ArgsException(); } } public void set(String s) { } public Object get() { return stringValue; } } private class IntegerArgum...\n",
      "\n",
      "‚Ä¢ 0.0564 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p3 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p3_c1\n",
      "  Fluent Python By Luciano Ramalho Copyright ¬© 2021 Luciano Gama de Sousa Ramalho. All rights reserved. Printed in the United States of America. Published by O‚ÄôReilly Media...\n",
      "\n",
      "‚úÖ vectorizer.pkl guardado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/ynd178zj0lsf9_dhk8rpqy200000gn/T/ipykernel_40050/4068670005.py:230: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Union\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    PointStruct, SparseVector, SparseVectorParams,\n",
    ")\n",
    "from qdrant_client import models as qm\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "SPARSE_NAME = \"text\"  \n",
    "\n",
    "def is_existing_file(p: Union[str, Path]) -> bool:\n",
    "    try:\n",
    "        return Path(p).exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def read_pdf_pages(pdf_path: Path) -> list[dict]:\n",
    "    r = PdfReader(str(pdf_path))\n",
    "    out = []\n",
    "    for i, page in enumerate(r.pages, start=1):\n",
    "        text = page.extract_text() or \"\"\n",
    "        out.append({\"page\": i, \"text\": text})\n",
    "    return out\n",
    "\n",
    "def chunk_text_old(text: str, chunk_size: int = 300, overlap: int = 50) -> list[str]:\n",
    "    toks = text.split()\n",
    "    out, start, n = [], 0, len(toks)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        ch = \" \".join(toks[start:end]).strip()\n",
    "        if ch:\n",
    "            out.append(ch)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "def csr_row_to_sparse(row_csr):\n",
    "    return row_csr.indices.tolist(), row_csr.data.tolist()\n",
    "\n",
    "def to_uuid_str(namespace: str, local_id: str) -> str:\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, f\"{namespace}:{local_id}\"))\n",
    "\n",
    "\n",
    "def build_corpus_from_documents(\n",
    "    documents: Iterable[Union[str, Path]],\n",
    "    chunk_size: int = 300,\n",
    "    overlap: int = 50,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - Path/str to PDF files\n",
    "      - raw text strings (treated as virtual docs)\n",
    "    Returns DataFrame: id, source, page, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for doc in documents:\n",
    "        if is_existing_file(doc):\n",
    "            p = Path(doc)\n",
    "            if p.suffix.lower() == \".pdf\":\n",
    "                pages = read_pdf_pages(p)\n",
    "                for pg in pages:\n",
    "                    for j, c in enumerate(chunk_text(pg[\"text\"], chunk_size, overlap), start=1):\n",
    "                        rows.append({\n",
    "                            \"id\": f\"{p.name}:p{pg['page']}_c{j}\",\n",
    "                            \"source\": str(p),\n",
    "                            \"page\": int(pg[\"page\"]),\n",
    "                            \"text\": c,\n",
    "                        })\n",
    "            else:\n",
    "                txt = Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                for j, c in enumerate(chunk_text(txt, chunk_size, overlap), start=1):\n",
    "                    rows.append({\n",
    "                        \"id\": f\"{p.name}:c{j}\",\n",
    "                        \"source\": str(p),\n",
    "                        \"page\": None,\n",
    "                        \"text\": c,\n",
    "                    })\n",
    "        else:\n",
    "            txt = str(doc)\n",
    "            pseudo = f\"raw:{hash(txt) & 0xffffffff:x}\"\n",
    "            for j, c in enumerate(chunk_text(txt, chunk_size, overlap), start=1):\n",
    "                rows.append({\n",
    "                    \"id\": f\"{pseudo}:c{j}\",\n",
    "                    \"source\": pseudo,\n",
    "                    \"page\": None,\n",
    "                    \"text\": c,\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No chunks produced. Check your inputs.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_sparse_collection(client: QdrantClient, collection: str, sparse_name: str = SPARSE_NAME):\n",
    "    if client.collection_exists(collection):\n",
    "        client.delete_collection(collection)\n",
    "    client.create_collection(\n",
    "        collection_name=collection,\n",
    "        vectors_config={},  # no dense vectors\n",
    "        sparse_vectors_config={sparse_name: SparseVectorParams()},\n",
    "    )\n",
    "\n",
    "def upsert_dataframe_tfidf_sparse(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    df: pd.DataFrame,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "    namespace_for_uuid: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upserts df rows (id, source, page, text) using TF-IDF vectors into Qdrant.\n",
    "    Uses UUIDv5 *string* IDs derived from df[\"id\"] (and optional namespace).\n",
    "    \"\"\"\n",
    "    X = vectorizer.transform(df[\"text\"])  # CSR aligned with df\n",
    "    n = len(df)\n",
    "    total = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for b in range(total):\n",
    "        lo, hi = b * batch_size, min((b + 1) * batch_size, n)\n",
    "        points = []\n",
    "        for i in range(lo, hi):\n",
    "            local_id = str(df.iloc[i][\"id\"])  # human-readable\n",
    "            point_id = to_uuid_str(namespace_for_uuid or collection, local_id)\n",
    "            src = df.iloc[i][\"source\"]\n",
    "            page_raw = df.iloc[i][\"page\"]  # may be None/NaN/int\n",
    "            page_val = int(page_raw) if pd.notna(page_raw) else None\n",
    "            txt = df.iloc[i][\"text\"]\n",
    "            idx, vals = csr_row_to_sparse(X[i])\n",
    "\n",
    "            payload = {\"source\": src, \"chunk_id\": local_id, \"text\": txt}\n",
    "            if page_val is not None:\n",
    "                payload[\"page\"] = page_val\n",
    "\n",
    "            # named sparse vector goes in the \"vector\" dict\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=point_id,\n",
    "                    payload=payload,\n",
    "                    vector={sparse_name: SparseVector(indices=idx, values=vals)},\n",
    "                )\n",
    "            )\n",
    "        client.upsert(collection_name=collection, points=points)\n",
    "        print(f\"Upserted batch {b+1}/{total} ({hi - lo} points)\")\n",
    "\n",
    "\n",
    "def build_and_index(\n",
    "    documents: Iterable[Union[str, Path]],\n",
    "    collection: str = COLLECTION,\n",
    "    chunk_size: int = BATCH_SIZE,\n",
    "    overlap: int = 50,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "):\n",
    "    # 1) Build corpus\n",
    "    df = build_corpus_from_documents(documents, chunk_size, overlap)\n",
    "    print(\"Corpus shape:\", df.shape)\n",
    "\n",
    "    # 2) Fit TF-IDF on ALL texts (shared vocab)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "    vectorizer.fit(df[\"text\"])\n",
    "\n",
    "    # 3) Qdrant client (HTTP; modern API)\n",
    "    client = QdrantClient(\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=False,\n",
    "        # Set True when client/server majors/minors are aligned\n",
    "        check_compatibility=False,\n",
    "    )\n",
    "\n",
    "    # 4) Create collection (only sparse)\n",
    "    create_sparse_collection(client, collection, sparse_name)\n",
    "\n",
    "    # 5) Upsert\n",
    "    upsert_dataframe_tfidf_sparse(\n",
    "        client=client,\n",
    "        collection=collection,\n",
    "        df=df,\n",
    "        vectorizer=vectorizer,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sparse_name=sparse_name,\n",
    "        namespace_for_uuid=collection,\n",
    "    )\n",
    "\n",
    "    return client, vectorizer, df\n",
    "\n",
    "# =========================\n",
    "# Retrieval (use deprecated but stable .search to avoid resolver issues)\n",
    "# =========================\n",
    "def query_tfidf(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "    source_filter: str | None = None,\n",
    "    page_eq: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses client.search (deprecated) with top-level models to avoid the fast-embed resolver\n",
    "    rejecting http.models.* types on your current client version.\n",
    "    \"\"\"\n",
    "    # TF-IDF ‚Üí sparse indices/values\n",
    "    q = vectorizer.transform([query])\n",
    "    idx = q.indices.tolist()\n",
    "    vals = q.data.tolist()\n",
    "\n",
    "    # Optional filters\n",
    "    q_filter = None\n",
    "    must = []\n",
    "    if source_filter:\n",
    "        must.append(qm.FieldCondition(key=\"source\", match=qm.Match(value=source_filter)))\n",
    "    if page_eq is not None:\n",
    "        must.append(qm.FieldCondition(key=\"page\", match=qm.Match(value=int(page_eq))))\n",
    "    if must:\n",
    "        q_filter = qm.Filter(must=must)\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=qm.NamedSparseVector(\n",
    "            name=sparse_name,\n",
    "            vector=qm.SparseVector(indices=idx, values=vals),\n",
    "        ),\n",
    "        query_filter=q_filter,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    print(f\"\\nüîé '{query}' ‚Üí top {top_k}\\n\")\n",
    "    for p in results:\n",
    "        pl = p.payload or {}\n",
    "        out.append({\n",
    "            \"id\": str(p.id),\n",
    "            \"score\": p.score,\n",
    "            \"source\": pl.get(\"source\"),\n",
    "            \"page\": pl.get(\"page\"),\n",
    "            \"chunk_id\": pl.get(\"chunk_id\"),\n",
    "            \"text\": pl.get(\"text\"),\n",
    "        })\n",
    "        print(f\"‚Ä¢ {p.score:.4f} | {pl.get('source')} | p{pl.get('page')} | {pl.get('chunk_id')}\")\n",
    "        print(f\"  {(pl.get('text') or '')[:170]}...\\n\")\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Example usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    docs = [\n",
    "        Path(\"./knowledge/cc_knowledge_book.pdf\"),\n",
    "        Path(\"./knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf\"),\n",
    "        \"This is an extra note about Clean Code principles to test multi-doc ingestion.\"\n",
    "    ]\n",
    "\n",
    "    client, vectorizer, df = build_and_index(\n",
    "        documents=docs,\n",
    "        collection=COLLECTION,\n",
    "        chunk_size=300,\n",
    "        overlap=50,\n",
    "    )\n",
    "\n",
    "    # Global retrieval\n",
    "    query_tfidf(client, COLLECTION, vectorizer, \"principios de c√≥digo limpio y mantenibilidad\", top_k=5)\n",
    "\n",
    "    import pickle\n",
    "    with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    print(\"‚úÖ vectorizer.pkl guardado\")\n",
    "    # Example with a filter by source and/or page\n",
    "    # query_tfidf(\n",
    "    #     client, COLLECTION, vectorizer,\n",
    "    #     query=\"naming functions and readability\",\n",
    "    #     top_k=5,\n",
    "    #     source_filter=str(Path(\"./knowledge/cc_knowledge_book.pdf\")),\n",
    "    #     page_eq=3,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import NamedSparseVector, SparseVector\n",
    "\n",
    "def search_tfidf(client: QdrantClient, collection_name: str, query: str, vectorizer, top_k: int = 5):\n",
    "    q = vectorizer.transform([query])\n",
    "    idx = q.indices.tolist()\n",
    "    vals = q.data.tolist()\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=NamedSparseVector(\n",
    "            name=\"text\",  \n",
    "            vector=SparseVector(indices=idx, values=vals)\n",
    "        ),\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüîç Query: '{query}' ‚Üí Top {top_k} results:\\n\")\n",
    "    for r in results:\n",
    "        print(f\"‚Ä¢ Score: {r.score:.4f} | Page: {r.payload.get('page')} | Chunk: {r.payload.get('chunk_id')}\")\n",
    "        print(f\"  {r.payload.get('text', '')[:180]}...\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Query: 'side efects' ‚Üí Top 5 results:\n",
      "\n",
      "‚Ä¢ Score: 0.1733 | Page: 75 | Chunk: cc_knowledge_book.pdf:p75_c1\n",
      "  44 Chapter 3: Functions Have No Side Effects Side effects are lies. Y our function promises to do one thing, but it also does other hidden things. Sometimes it will make unexpected...\n",
      "\n",
      "‚Ä¢ Score: 0.1192 | Page: 328 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p328_c1\n",
      "  object created or referenced on the right-hand side. And the object must exist before a name can be bound to it, as Example 6-2 proves. Example 6-2. Variables are bound to objects ...\n",
      "\n",
      "‚Ä¢ Score: 0.1062 | Page: 1384 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1384_c1\n",
      "  i. Pipelines of coroutines i. The Meaning of yield from i. Basic behavior of yield from ii. Exception handling in yield from j. Use Case: Coroutines for Discrete Event Simulation i...\n",
      "\n",
      "‚Ä¢ Score: 0.1048 | Page: 1061 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1061_c1\n",
      "  Figure 20-3. One possible architecture for a data system that combines several components. There are other industrial-strength Open Source products in each of those categories. Maj...\n",
      "\n",
      "‚Ä¢ Score: 0.0883 | Page: 1202 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1202_c1\n",
      "  When I told this story to Glyph Lefkowitz‚Äîfounder the Twisted project ‚Äîhe said that one of his priorities at the start of an asynchronous programming project is to decide which too...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/ynd178zj0lsf9_dhk8rpqy200000gn/T/ipykernel_40050/1117325604.py:9: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "\n",
    "results = search_tfidf(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION,\n",
    "    query=\"side efects\",\n",
    "    vectorizer=vectorizer,\n",
    "    top_k=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sauco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
