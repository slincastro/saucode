{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: pypdf in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (6.1.1)\n",
      "Requirement already satisfied: pandas in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pypdf pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client>=1.8.2 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (1.15.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (1.74.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (2.3.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (2.11.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from qdrant-client>=1.8.2) (2.5.0)\n",
      "Requirement already satisfied: anyio in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.8.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.8.2) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.8.2) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client>=1.8.2) (0.4.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client>=1.8.2) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"qdrant-client>=1.8.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: qdrant-client\n",
      "Version: 1.15.1\n",
      "Summary: Client library for the Qdrant vector search engine\n",
      "Home-page: \n",
      "Author: Andrey Vasnetsov\n",
      "Author-email: andrey@qdrant.tech\n",
      "License: Apache-2.0\n",
      "Location: /Users/slincastro/Projects/Master_IA/final_project/saucode/sauco-env/lib/python3.12/site-packages\n",
      "Requires: grpcio, httpx, numpy, portalocker, protobuf, pydantic, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show qdrant-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'my_collection_tfidf' deleted.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "COLLECTION = \"my_collection_tfidf\"\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False, check_compatibility=False)\n",
    "\n",
    "if client.collection_exists(COLLECTION):\n",
    "    client.delete_collection(COLLECTION)\n",
    "    print(f\"Collection '{COLLECTION}' deleted.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: (668, 3)\n",
      "      id  page                                               text\n",
      "0  p2_c1     2                                         Clean Code\n",
      "1  p3_c1     3  Robert C. Martin Series The mission of this se...\n",
      "Batch 1: upserted 512 points\n",
      "Batch 2: upserted 156 points\n",
      "Upserted 668 chunks in collection: my_collection_tfidf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/ynd178zj0lsf9_dhk8rpqy200000gn/T/ipykernel_47867/624216779.py:105: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import math, uuid\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    PointStruct, SparseVector, SparseVectorParams, NamedSparseVector\n",
    ")\n",
    "\n",
    "PDF_PATH = Path(\"./knowledge/cc_knowledge_book.pdf\")\n",
    "COLLECTION = \"my_collection_tfidf\"\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "def csr_row_to_sparse(row_csr):\n",
    "    return row_csr.indices.tolist(), row_csr.data.tolist()\n",
    "\n",
    "# ⬇️ ID estable: UUID v5 a partir del string original (p.ej. \"p2_c1\")\n",
    "def to_uuid(s: str) -> uuid.UUID:\n",
    "    # usa el nombre de la colección para que sea único por colección\n",
    "    return uuid.uuid5(uuid.NAMESPACE_URL, f\"{COLLECTION}:{s}\")\n",
    "\n",
    "def read_pdf_pages(pdf_path: Path) -> list[dict]:\n",
    "    r = PdfReader(str(pdf_path))\n",
    "    return [{\"page\": i, \"text\": (p.extract_text() or \"\")} for i, p in enumerate(r.pages, start=1)]\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 900, overlap: int = 150) -> list[str]:\n",
    "    toks = text.split()\n",
    "    out, start, n = [], 0, len(toks)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        ch = \" \".join(toks[start:end]).strip()\n",
    "        if ch: out.append(ch)\n",
    "        if end == n: break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"No se encontró el PDF en {PDF_PATH}.\")\n",
    "\n",
    "pages = read_pdf_pages(PDF_PATH)\n",
    "rows = []\n",
    "for p in pages:\n",
    "    for j, c in enumerate(chunk_text(p[\"text\"], chunk_size=300, overlap=50), start=1):\n",
    "        rows.append({\"id\": f\"p{p['page']}_c{j}\", \"page\": p[\"page\"], \"text\": c})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"Corpus:\", df.shape)\n",
    "print(df.head(2))\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    "    prefer_grpc=False,\n",
    "    # quita esto si ya alineaste servidor y cliente a la misma mayor (ideal)\n",
    "    check_compatibility=False,\n",
    ")\n",
    "\n",
    "# Mejor que recreate_collection (deprecado)\n",
    "if client.collection_exists(COLLECTION):\n",
    "    client.delete_collection(COLLECTION)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION,\n",
    "    vectors_config={},  # sin densos\n",
    "    sparse_vectors_config={\"text\": SparseVectorParams()},\n",
    ")\n",
    "\n",
    "# Upsert en lotes: USA UUID en id, y guarda el id original en payload\n",
    "n = len(df)\n",
    "for b in range((n + BATCH_SIZE - 1) // BATCH_SIZE):\n",
    "    lo, hi = b * BATCH_SIZE, min((b + 1) * BATCH_SIZE, n)\n",
    "    points = []\n",
    "    for i in range(lo, hi):\n",
    "        orig_id = str(df.iloc[i][\"id\"])           # \"p2_c1\"\n",
    "        qid = to_uuid(orig_id)                     # UUID estable\n",
    "        page = int(df.iloc[i][\"page\"])\n",
    "        txt = df.iloc[i][\"text\"]\n",
    "        idx, vals = csr_row_to_sparse(X[i])\n",
    "\n",
    "        points.append(\n",
    "            PointStruct(\n",
    "                id=str(qid),  # <-- UUID, válido para Qdrant\n",
    "                payload={\n",
    "                    \"page\": page,\n",
    "                    \"chunk_id\": orig_id,          # conserva tu id original como metadata\n",
    "                    \"text\": txt\n",
    "                },\n",
    "                # sparse nombrado \"text\" va en el dict \"vector\"\n",
    "                vector={\"text\": SparseVector(indices=idx, values=vals)},\n",
    "            )\n",
    "        )\n",
    "    client.upsert(collection_name=COLLECTION, points=points)\n",
    "    print(f\"Batch {b+1}: upserted {hi - lo} points\")\n",
    "\n",
    "print(f\"Upserted {n} chunks in collection: {COLLECTION}\")\n",
    "\n",
    "# Búsqueda\n",
    "def search_tfidf(query: str, top_k: int = 5):\n",
    "    q = vectorizer.transform([query])\n",
    "    q_idx, q_vals = csr_row_to_sparse(q[0])\n",
    "    hits = client.search(\n",
    "        collection_name=COLLECTION,\n",
    "        query_vector=NamedSparseVector(\n",
    "            name=\"text\",\n",
    "            vector=SparseVector(indices=q_idx, values=q_vals),\n",
    "        ),\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "    return [\n",
    "        {\"id\": str(h.id), \"score\": h.score, \"page\": h.payload.get(\"page\"),\n",
    "         \"chunk_id\": h.payload.get(\"chunk_id\"), \"text\": h.payload.get(\"text\")}\n",
    "        for h in hits\n",
    "    ]\n",
    "\n",
    "for r in search_tfidf(\"introducción conceptos clave\", top_k=5):\n",
    "    print(r[\"score\"], r[\"chunk_id\"], f\"(p{r['page']})\", (r[\"text\"] or \"\")[:120], \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus shape: (2144, 4)\n",
      "Upserted batch 1/5 (512 points)\n",
      "Upserted batch 2/5 (512 points)\n",
      "Upserted batch 3/5 (512 points)\n",
      "Upserted batch 4/5 (512 points)\n",
      "Upserted batch 5/5 (96 points)\n",
      "\n",
      "🔎 'principios de código limpio y mantenibilidad' → top 5\n",
      "\n",
      "• 0.1600 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p1082 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1082_c2\n",
      "  python3 flags_threadpool.py DE BD CN JP ID EG NG BR RU CD IR MX US PH FR PK VN IN ET TR 20 flags downloaded in 1.37s $ python3 flags_threadpool.py EG BR FR IN BD JP DE RU...\n",
      "\n",
      "• 0.1311 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p1082 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1082_c1\n",
      "  To make this last point with code, I wrote three simple programs to download images of 20 country flags from the Web. The first one, flags.py, runs sequentially: it only ...\n",
      "\n",
      "• 0.0932 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p641 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p641_c2\n",
      "  once you’ve bagged it, for example, specific observable traits (not all of them—plumage, for example, is de minimis in such a context), mostly texture and flavor (old-fas...\n",
      "\n",
      "• 0.0801 | knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf | p1083 | Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1083_c1\n",
      "  RU CN BR IN FR BD TR EG VN IR PH CD ET ID NG DE JP PK MX US 20 flags downloaded in 1.27s $ python3 flags_asyncio.py RU IN ID DE BR VN PK MX US IR ET EG NG BD FR CN JP PH ...\n",
      "\n",
      "• 0.0582 | knowledge/cc_knowledge_book.pdf | p267 | cc_knowledge_book.pdf:p267_c1\n",
      "  236 Chapter 14: Successive Reﬁnement throw new ArgsException(); } } public void set(String s) { } public Object get() { return stringValue; } } private class IntegerArgum...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/ynd178zj0lsf9_dhk8rpqy200000gn/T/ipykernel_47867/935563858.py:247: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Union\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "# HTTP models → for upserts/collection schema\n",
    "from qdrant_client.http.models import (\n",
    "    PointStruct, SparseVector, SparseVectorParams,\n",
    ")\n",
    "# Top-level models → for search (resolver-friendly in your client)\n",
    "from qdrant_client import models as qm\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "COLLECTION = \"my_collection_tfidf\"\n",
    "BATCH_SIZE = 512\n",
    "SPARSE_NAME = \"text\"  # named sparse vector key in Qdrant\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def is_existing_file(p: Union[str, Path]) -> bool:\n",
    "    try:\n",
    "        return Path(p).exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def read_pdf_pages(pdf_path: Path) -> list[dict]:\n",
    "    r = PdfReader(str(pdf_path))\n",
    "    out = []\n",
    "    for i, page in enumerate(r.pages, start=1):\n",
    "        text = page.extract_text() or \"\"\n",
    "        out.append({\"page\": i, \"text\": text})\n",
    "    return out\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50) -> list[str]:\n",
    "    toks = text.split()\n",
    "    out, start, n = [], 0, len(toks)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        ch = \" \".join(toks[start:end]).strip()\n",
    "        if ch:\n",
    "            out.append(ch)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "def csr_row_to_sparse(row_csr):\n",
    "    return row_csr.indices.tolist(), row_csr.data.tolist()\n",
    "\n",
    "def to_uuid_str(namespace: str, local_id: str) -> str:\n",
    "    # Deterministic UUIDv5 -> string (Qdrant accepts str IDs)\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_URL, f\"{namespace}:{local_id}\"))\n",
    "\n",
    "# =========================\n",
    "# Build corpus from many docs\n",
    "# =========================\n",
    "def build_corpus_from_documents(\n",
    "    documents: Iterable[Union[str, Path]],\n",
    "    chunk_size: int = 300,\n",
    "    overlap: int = 50,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - Path/str to PDF files\n",
    "      - raw text strings (treated as virtual docs)\n",
    "    Returns DataFrame: id, source, page, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for doc in documents:\n",
    "        if is_existing_file(doc):\n",
    "            p = Path(doc)\n",
    "            if p.suffix.lower() == \".pdf\":\n",
    "                pages = read_pdf_pages(p)\n",
    "                for pg in pages:\n",
    "                    for j, c in enumerate(chunk_text(pg[\"text\"], chunk_size, overlap), start=1):\n",
    "                        rows.append({\n",
    "                            \"id\": f\"{p.name}:p{pg['page']}_c{j}\",\n",
    "                            \"source\": str(p),\n",
    "                            \"page\": int(pg[\"page\"]),\n",
    "                            \"text\": c,\n",
    "                        })\n",
    "            else:\n",
    "                txt = Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                for j, c in enumerate(chunk_text(txt, chunk_size, overlap), start=1):\n",
    "                    rows.append({\n",
    "                        \"id\": f\"{p.name}:c{j}\",\n",
    "                        \"source\": str(p),\n",
    "                        \"page\": None,\n",
    "                        \"text\": c,\n",
    "                    })\n",
    "        else:\n",
    "            # Treat plain string as a virtual doc\n",
    "            txt = str(doc)\n",
    "            pseudo = f\"raw:{hash(txt) & 0xffffffff:x}\"\n",
    "            for j, c in enumerate(chunk_text(txt, chunk_size, overlap), start=1):\n",
    "                rows.append({\n",
    "                    \"id\": f\"{pseudo}:c{j}\",\n",
    "                    \"source\": pseudo,\n",
    "                    \"page\": None,\n",
    "                    \"text\": c,\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No chunks produced. Check your inputs.\")\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# Qdrant collection helpers\n",
    "# =========================\n",
    "def create_sparse_collection(client: QdrantClient, collection: str, sparse_name: str = SPARSE_NAME):\n",
    "    # Sparse vectors must be named in the schema\n",
    "    if client.collection_exists(collection):\n",
    "        client.delete_collection(collection)\n",
    "    client.create_collection(\n",
    "        collection_name=collection,\n",
    "        vectors_config={},  # no dense vectors\n",
    "        sparse_vectors_config={sparse_name: SparseVectorParams()},\n",
    "    )\n",
    "\n",
    "def upsert_dataframe_tfidf_sparse(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    df: pd.DataFrame,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "    namespace_for_uuid: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upserts df rows (id, source, page, text) using TF-IDF vectors into Qdrant.\n",
    "    Uses UUIDv5 *string* IDs derived from df[\"id\"] (and optional namespace).\n",
    "    \"\"\"\n",
    "    X = vectorizer.transform(df[\"text\"])  # CSR aligned with df\n",
    "    n = len(df)\n",
    "    total = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for b in range(total):\n",
    "        lo, hi = b * batch_size, min((b + 1) * batch_size, n)\n",
    "        points = []\n",
    "        for i in range(lo, hi):\n",
    "            local_id = str(df.iloc[i][\"id\"])  # human-readable\n",
    "            point_id = to_uuid_str(namespace_for_uuid or collection, local_id)\n",
    "            src = df.iloc[i][\"source\"]\n",
    "            page_raw = df.iloc[i][\"page\"]  # may be None/NaN/int\n",
    "            page_val = int(page_raw) if pd.notna(page_raw) else None\n",
    "            txt = df.iloc[i][\"text\"]\n",
    "            idx, vals = csr_row_to_sparse(X[i])\n",
    "\n",
    "            payload = {\"source\": src, \"chunk_id\": local_id, \"text\": txt}\n",
    "            if page_val is not None:\n",
    "                payload[\"page\"] = page_val\n",
    "\n",
    "            # named sparse vector goes in the \"vector\" dict\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=point_id,\n",
    "                    payload=payload,\n",
    "                    vector={sparse_name: SparseVector(indices=idx, values=vals)},\n",
    "                )\n",
    "            )\n",
    "        client.upsert(collection_name=collection, points=points)\n",
    "        print(f\"Upserted batch {b+1}/{total} ({hi - lo} points)\")\n",
    "\n",
    "# =========================\n",
    "# Build+Index end-to-end\n",
    "# =========================\n",
    "def build_and_index(\n",
    "    documents: Iterable[Union[str, Path]],\n",
    "    collection: str = COLLECTION,\n",
    "    chunk_size: int = 300,\n",
    "    overlap: int = 50,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "):\n",
    "    # 1) Build corpus\n",
    "    df = build_corpus_from_documents(documents, chunk_size, overlap)\n",
    "    print(\"Corpus shape:\", df.shape)\n",
    "\n",
    "    # 2) Fit TF-IDF on ALL texts (shared vocab)\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "    vectorizer.fit(df[\"text\"])\n",
    "\n",
    "    # 3) Qdrant client (HTTP; modern API)\n",
    "    client = QdrantClient(\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=False,\n",
    "        # Set True when client/server majors/minors are aligned\n",
    "        check_compatibility=False,\n",
    "    )\n",
    "\n",
    "    # 4) Create collection (only sparse)\n",
    "    create_sparse_collection(client, collection, sparse_name)\n",
    "\n",
    "    # 5) Upsert\n",
    "    upsert_dataframe_tfidf_sparse(\n",
    "        client=client,\n",
    "        collection=collection,\n",
    "        df=df,\n",
    "        vectorizer=vectorizer,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sparse_name=sparse_name,\n",
    "        namespace_for_uuid=collection,\n",
    "    )\n",
    "\n",
    "    return client, vectorizer, df\n",
    "\n",
    "# =========================\n",
    "# Retrieval (use deprecated but stable .search to avoid resolver issues)\n",
    "# =========================\n",
    "def query_tfidf(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    vectorizer: TfidfVectorizer,\n",
    "    query: str,\n",
    "    top_k: int = 5,\n",
    "    sparse_name: str = SPARSE_NAME,\n",
    "    source_filter: str | None = None,\n",
    "    page_eq: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses client.search (deprecated) with top-level models to avoid the fast-embed resolver\n",
    "    rejecting http.models.* types on your current client version.\n",
    "    \"\"\"\n",
    "    # TF-IDF → sparse indices/values\n",
    "    q = vectorizer.transform([query])\n",
    "    idx = q.indices.tolist()\n",
    "    vals = q.data.tolist()\n",
    "\n",
    "    # Optional filters\n",
    "    q_filter = None\n",
    "    must = []\n",
    "    if source_filter:\n",
    "        must.append(qm.FieldCondition(key=\"source\", match=qm.Match(value=source_filter)))\n",
    "    if page_eq is not None:\n",
    "        must.append(qm.FieldCondition(key=\"page\", match=qm.Match(value=int(page_eq))))\n",
    "    if must:\n",
    "        q_filter = qm.Filter(must=must)\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection,\n",
    "        query_vector=qm.NamedSparseVector(\n",
    "            name=sparse_name,\n",
    "            vector=qm.SparseVector(indices=idx, values=vals),\n",
    "        ),\n",
    "        query_filter=q_filter,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    print(f\"\\n🔎 '{query}' → top {top_k}\\n\")\n",
    "    for p in results:\n",
    "        pl = p.payload or {}\n",
    "        out.append({\n",
    "            \"id\": str(p.id),\n",
    "            \"score\": p.score,\n",
    "            \"source\": pl.get(\"source\"),\n",
    "            \"page\": pl.get(\"page\"),\n",
    "            \"chunk_id\": pl.get(\"chunk_id\"),\n",
    "            \"text\": pl.get(\"text\"),\n",
    "        })\n",
    "        print(f\"• {p.score:.4f} | {pl.get('source')} | p{pl.get('page')} | {pl.get('chunk_id')}\")\n",
    "        print(f\"  {(pl.get('text') or '')[:170]}...\\n\")\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Example usage\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    docs = [\n",
    "        Path(\"./knowledge/cc_knowledge_book.pdf\"),\n",
    "        Path(\"./knowledge/Fluent.Python.2nd.Edition.(z-lib.org).pdf\"),\n",
    "        \"This is an extra note about Clean Code principles to test multi-doc ingestion.\"\n",
    "    ]\n",
    "\n",
    "    client, vectorizer, df = build_and_index(\n",
    "        documents=docs,\n",
    "        collection=COLLECTION,\n",
    "        chunk_size=300,\n",
    "        overlap=50,\n",
    "    )\n",
    "\n",
    "    # Global retrieval\n",
    "    query_tfidf(client, COLLECTION, vectorizer, \"principios de código limpio y mantenibilidad\", top_k=5)\n",
    "\n",
    "    # Example with a filter by source and/or page\n",
    "    # query_tfidf(\n",
    "    #     client, COLLECTION, vectorizer,\n",
    "    #     query=\"naming functions and readability\",\n",
    "    #     top_k=5,\n",
    "    #     source_filter=str(Path(\"./knowledge/cc_knowledge_book.pdf\")),\n",
    "    #     page_eq=3,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import NamedSparseVector, SparseVector\n",
    "\n",
    "def search_tfidf(client: QdrantClient, collection_name: str, query: str, vectorizer, top_k: int = 5):\n",
    "    # Convert query text into TF-IDF sparse vector\n",
    "    q = vectorizer.transform([query])\n",
    "    idx = q.indices.tolist()\n",
    "    vals = q.data.tolist()\n",
    "\n",
    "    # Perform sparse vector search\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=NamedSparseVector(\n",
    "            name=\"text\",  # must match your sparse_vectors_config key\n",
    "            vector=SparseVector(indices=idx, values=vals)\n",
    "        ),\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    # Pretty-print results\n",
    "    print(f\"\\n🔍 Query: '{query}' → Top {top_k} results:\\n\")\n",
    "    for r in results:\n",
    "        print(f\"• Score: {r.score:.4f} | Page: {r.payload.get('page')} | Chunk: {r.payload.get('chunk_id')}\")\n",
    "        print(f\"  {r.payload.get('text', '')[:180]}...\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: 'side efects' → Top 5 results:\n",
      "\n",
      "• Score: 0.1718 | Page: 75 | Chunk: cc_knowledge_book.pdf:p75_c1\n",
      "  44 Chapter 3: Functions Have No Side Effects Side effects are lies. Y our function promises to do one thing, but it also does other hidden things. Sometimes it will make unexpected...\n",
      "\n",
      "• Score: 0.1188 | Page: 328 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p328_c1\n",
      "  object created or referenced on the right-hand side. And the object must exist before a name can be bound to it, as Example 6-2 proves. Example 6-2. Variables are bound to objects ...\n",
      "\n",
      "• Score: 0.1059 | Page: 1384 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1384_c1\n",
      "  i. Pipelines of coroutines i. The Meaning of yield from i. Basic behavior of yield from ii. Exception handling in yield from j. Use Case: Coroutines for Discrete Event Simulation i...\n",
      "\n",
      "• Score: 0.1045 | Page: 1061 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1061_c1\n",
      "  Figure 20-3. One possible architecture for a data system that combines several components. There are other industrial-strength Open Source products in each of those categories. Maj...\n",
      "\n",
      "• Score: 0.0862 | Page: 1202 | Chunk: Fluent.Python.2nd.Edition.(z-lib.org).pdf:p1202_c1\n",
      "  When I told this story to Glyph Lefkowitz—founder the Twisted project —he said that one of his priorities at the start of an asynchronous programming project is to decide which too...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/ynd178zj0lsf9_dhk8rpqy200000gn/T/ipykernel_47867/2797194691.py:11: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# reconnect client if needed\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "\n",
    "# your collection and fitted TF-IDF vectorizer\n",
    "COLLECTION = \"my_collection_tfidf\"\n",
    "\n",
    "results = search_tfidf(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION,\n",
    "    query=\"side efects\",\n",
    "    vectorizer=vectorizer,\n",
    "    top_k=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sauco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
