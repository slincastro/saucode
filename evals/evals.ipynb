{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "\n",
    "We are going to excecute a set of tests over a ser of excercises , get the improved code and save a historical in a dataframe.\n",
    "\n",
    "Excercise 1 : Fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.002s\n",
      "\n",
      "OK\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empezando el calculo de algo...\n",
      "No se hace nada jeje\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [0, 1, 1, 2, 3]\n",
      "Empezando el calculo de algo...\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n",
      "Empezando el calculo de algo...\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [0, 1]\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [0, 1, 1, 2, 3]\n",
      "Empezando el calculo de algo...\n",
      "Resultado (no optimizado): [5, 10, 0, 1]\n",
      "Empezando el calculo de algo...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import sys\n",
    "sys.path.append('src/excercise1-fibonacci')\n",
    "from fibonacci_test import TestFibonacci\n",
    "\n",
    "# Ejecutar todas las pruebas\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "\n",
    "# O ejecutar pruebas específicas\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(TestFibonacci('test_empty_sequence'))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Test Excecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name</th>\n",
       "      <th>status</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_c_parameter_false</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_custom_start_values</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_empty_sequence</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_large_sequence</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_single_element</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_standard_fibonacci</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_two_elements</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_with_args_kwargs</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_with_provided_x</td>\n",
       "      <td>PASS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUMMARY</td>\n",
       "      <td>9/9 (100.00%)</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  test_name         status  execution_time error_message\n",
       "0    test_c_parameter_false           PASS               0          None\n",
       "1  test_custom_start_values           PASS               0          None\n",
       "2       test_empty_sequence           PASS               0          None\n",
       "3       test_large_sequence           PASS               0          None\n",
       "4       test_single_element           PASS               0          None\n",
       "5   test_standard_fibonacci           PASS               0          None\n",
       "6         test_two_elements           PASS               0          None\n",
       "7     test_with_args_kwargs           PASS               0          None\n",
       "8      test_with_provided_x           PASS               0          None\n",
       "9                   SUMMARY  9/9 (100.00%)               0          None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porcentaje de éxito: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "\n",
    "# Añadir la ruta al directorio que contiene los archivos de prueba\n",
    "sys.path.append('src/excercise1-fibonacci')\n",
    "from fibonacci_test import TestFibonacci\n",
    "\n",
    "# Clase personalizada para capturar resultados de pruebas\n",
    "class TestResultCollector(unittest.TextTestResult):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_results = []\n",
    "    \n",
    "    def addSuccess(self, test):\n",
    "        super().addSuccess(test)\n",
    "        self.test_results.append({\n",
    "            'test_name': test.id().split('.')[-1],\n",
    "            'status': 'PASS',\n",
    "            'execution_time': getattr(test, 'execution_time', 0),\n",
    "            'error_message': None\n",
    "        })\n",
    "    \n",
    "    def addFailure(self, test, err):\n",
    "        super().addFailure(test, err)\n",
    "        self.test_results.append({\n",
    "            'test_name': test.id().split('.')[-1],\n",
    "            'status': 'FAIL',\n",
    "            'execution_time': getattr(test, 'execution_time', 0),\n",
    "            'error_message': str(err[1])\n",
    "        })\n",
    "    \n",
    "    def addError(self, test, err):\n",
    "        super().addError(test, err)\n",
    "        self.test_results.append({\n",
    "            'test_name': test.id().split('.')[-1],\n",
    "            'status': 'ERROR',\n",
    "            'execution_time': getattr(test, 'execution_time', 0),\n",
    "            'error_message': str(err[1])\n",
    "        })\n",
    "\n",
    "class TimedTestCase(unittest.TestCase):\n",
    "    def run(self, result=None):\n",
    "        start_time = time.time()\n",
    "        super().run(result)\n",
    "        self.execution_time = time.time() - start_time\n",
    "\n",
    "class TimedTestFibonacci(TestFibonacci, TimedTestCase):\n",
    "    pass\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TimedTestFibonacci)\n",
    "\n",
    "output = StringIO()\n",
    "with contextlib.redirect_stdout(output):\n",
    "    runner = unittest.TextTestRunner(resultclass=TestResultCollector)\n",
    "    result = runner.run(suite)\n",
    "\n",
    "test_results_df = pd.DataFrame(result.test_results)\n",
    "\n",
    "total_tests = len(test_results_df)\n",
    "passed_tests = len(test_results_df[test_results_df['status'] == 'PASS'])\n",
    "success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'test_name': 'SUMMARY',\n",
    "    'status': f'{passed_tests}/{total_tests} ({success_rate:.2f}%)',\n",
    "    'execution_time': test_results_df['execution_time'].sum(),\n",
    "    'error_message': None\n",
    "}])\n",
    "\n",
    "# Combinar resultados y resumen\n",
    "results_df = pd.concat([test_results_df, summary], ignore_index=True)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "display(results_df)\n",
    "\n",
    "# Mostrar solo el porcentaje de éxito\n",
    "print(f\"\\nPorcentaje de éxito: {success_rate:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Excecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.002s\n",
      "\n",
      "OK\n",
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.002s\n",
      "\n",
      "OK\n",
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>iteration</th>\n",
       "      <th>tests</th>\n",
       "      <th>percentage_of_success</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/excercise1-fibonacci/fibonacci_test</td>\n",
       "      <td>1</td>\n",
       "      <td>9/9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>src/excercise1-fibonacci/fibonacci_test</td>\n",
       "      <td>2</td>\n",
       "      <td>9/9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>src/excercise1-fibonacci/fibonacci_test</td>\n",
       "      <td>3</td>\n",
       "      <td>9/9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file  iteration tests  \\\n",
       "0  src/excercise1-fibonacci/fibonacci_test          1   9/9   \n",
       "1  src/excercise1-fibonacci/fibonacci_test          2   9/9   \n",
       "2  src/excercise1-fibonacci/fibonacci_test          3   9/9   \n",
       "\n",
       "   percentage_of_success  execution_time  \n",
       "0                  100.0               0  \n",
       "1                  100.0               0  \n",
       "2                  100.0               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "import inspect\n",
    "\n",
    "def run_tests_for_file(file_path, iteration=1):\n",
    "    \"\"\"\n",
    "    Ejecuta las pruebas para un archivo específico y devuelve los resultados.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo de prueba (sin extensión .py)\n",
    "        iteration: Número de iteración\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con los resultados\n",
    "    \"\"\"\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    if file_dir and file_dir not in sys.path:\n",
    "        sys.path.append(file_dir)\n",
    "    \n",
    "    try:\n",
    "        test_module = importlib.import_module(file_name)\n",
    "        \n",
    "        test_classes = []\n",
    "        for name, obj in inspect.getmembers(test_module):\n",
    "            if inspect.isclass(obj) and issubclass(obj, unittest.TestCase) and obj != unittest.TestCase:\n",
    "                test_classes.append(obj)\n",
    "        \n",
    "        if not test_classes:\n",
    "            return pd.DataFrame({\n",
    "                'file': [file_path],\n",
    "                'iteration': [iteration],\n",
    "                'tests': ['0/0'],\n",
    "                'percentage_of_success': [0.0],\n",
    "                'execution_time': [0.0]\n",
    "            })\n",
    "        \n",
    "        all_results = []\n",
    "        total_passed = 0\n",
    "        total_tests = 0\n",
    "        total_time = 0\n",
    "        \n",
    "        for test_class in test_classes:\n",
    "            class TimedTestCase(test_class):\n",
    "                def run(self, result=None):\n",
    "                    start_time = time.time()\n",
    "                    super().run(result)\n",
    "                    self.execution_time = time.time() - start_time\n",
    "            \n",
    "            class TestResultCollector(unittest.TextTestResult):\n",
    "                def __init__(self, *args, **kwargs):\n",
    "                    super().__init__(*args, **kwargs)\n",
    "                    self.test_results = []\n",
    "                \n",
    "                def addSuccess(self, test):\n",
    "                    super().addSuccess(test)\n",
    "                    self.test_results.append({\n",
    "                        'test_name': test.id().split('.')[-1],\n",
    "                        'status': 'PASS',\n",
    "                        'execution_time': getattr(test, 'execution_time', 0),\n",
    "                        'error_message': None\n",
    "                    })\n",
    "                \n",
    "                def addFailure(self, test, err):\n",
    "                    super().addFailure(test, err)\n",
    "                    self.test_results.append({\n",
    "                        'test_name': test.id().split('.')[-1],\n",
    "                        'status': 'FAIL',\n",
    "                        'execution_time': getattr(test, 'execution_time', 0),\n",
    "                        'error_message': str(err[1])\n",
    "                    })\n",
    "                \n",
    "                def addError(self, test, err):\n",
    "                    super().addError(test, err)\n",
    "                    self.test_results.append({\n",
    "                        'test_name': test.id().split('.')[-1],\n",
    "                        'status': 'ERROR',\n",
    "                        'execution_time': getattr(test, 'execution_time', 0),\n",
    "                        'error_message': str(err[1])\n",
    "                    })\n",
    "            \n",
    "            suite = unittest.TestLoader().loadTestsFromTestCase(TimedTestCase)\n",
    "            \n",
    "            output = StringIO()\n",
    "            with contextlib.redirect_stdout(output):\n",
    "                runner = unittest.TextTestRunner(resultclass=TestResultCollector)\n",
    "                result = runner.run(suite)\n",
    "            \n",
    "            class_results = result.test_results\n",
    "            all_results.extend(class_results)\n",
    "            \n",
    "            class_passed = len([r for r in class_results if r['status'] == 'PASS'])\n",
    "            class_total = len(class_results)\n",
    "            class_time = sum(r['execution_time'] for r in class_results)\n",
    "            \n",
    "            total_passed += class_passed\n",
    "            total_tests += class_total\n",
    "            total_time += class_time\n",
    "        \n",
    "        success_percentage = (total_passed / total_tests * 100) if total_tests > 0 else 0\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'file': [file_path],\n",
    "            'iteration': [iteration],\n",
    "            'tests': [f\"{total_passed}/{total_tests}\"],\n",
    "            'percentage_of_success': [success_percentage],\n",
    "            'execution_time': [total_time]\n",
    "        })\n",
    "        \n",
    "        return summary_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        return pd.DataFrame({\n",
    "            'file': [file_path],\n",
    "            'iteration': [iteration],\n",
    "            'tests': ['ERROR'],\n",
    "            'percentage_of_success': [0.0],\n",
    "            'execution_time': [0.0],\n",
    "            'error': [str(e)]\n",
    "        })\n",
    "\n",
    "def run_tests_for_multiple_files(file_paths, iterations=1):\n",
    "    \"\"\"\n",
    "    Ejecuta pruebas para múltiples archivos y devuelve un DataFrame consolidado.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: Lista de rutas a archivos de prueba (sin extensión .py)\n",
    "        iterations: Número de iteraciones para cada archivo\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame consolidado con todos los resultados\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        for i in range(1, iterations + 1):\n",
    "            result_df = run_tests_for_file(file_path, i)\n",
    "            all_results.append(result_df)\n",
    "    \n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['file', 'iteration', 'tests', 'percentage_of_success', 'execution_time'])\n",
    "\n",
    "\n",
    "test_files = ['src/excercise1-fibonacci/fibonacci_test']  \n",
    "iterations = 3  \n",
    "\n",
    "results_df = run_tests_for_multiple_files(test_files, iterations)\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "# Opcional: guardar resultados en CSV\n",
    "# results_df.to_csv('test_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
