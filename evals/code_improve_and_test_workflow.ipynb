{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b91b8b",
   "metadata": {},
   "source": [
    "# Code Improvement + Test Runner Workflow\n",
    "\n",
    "Este notebook implementa un flujo completo en **Python** para:\n",
    "1) Respaldar archivos de código, leer su contenido y enviarlo a una API de mejora.\n",
    "2) Ejecutar pruebas `unittest` asociadas y agregar los resultados **en la misma fila** del DataFrame.\n",
    "\n",
    "## Entrada esperada\n",
    "Una lista de objetos con la forma:\n",
    "```python\n",
    "items = [\n",
    "    {\"file\": \"path/al/archivo.py\", \"test\": \"ruta/al/test_module_sin_extension\", \"iterations\": 3},\n",
    "    {\"file\": \"otro/archivo.py\", \"test\": \"otro/test_module\", \"iterations\": 1},\n",
    "]\n",
    "```\n",
    "- **file**: ruta al archivo de código a mejorar (incluye `.py`).\n",
    "- **test**: ruta al *módulo de pruebas* **sin** la extensión `.py` (por ejemplo `src/excercise1-fibonacci/fibonacci_test`).\n",
    "- **iterations** *(opcional)*: número de veces que se envía el mismo archivo a la API. Cada envío genera **una fila** en el DataFrame.\n",
    "\n",
    "## Salida\n",
    "Un `DataFrame` con una fila por (archivo, test, iteración) incluyendo:\n",
    "- Metadatos del archivo, respaldo y contenido original.\n",
    "- Código mejorado y análisis devueltos por la API.\n",
    "- Métricas antes/después si la API las provee.\n",
    "- Resultados de pruebas (tests pasados/total, % éxito, tiempo, etc.).\n",
    "\n",
    "---\n",
    "⚙️ **Nota**: Configura `API_URL` con la URL de tu servicio (por defecto `http://127.0.0.1:8000/improve`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d602dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH actualizado con: /Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Obtiene la ruta absoluta al directorio 'src'\n",
    "project_root = os.path.abspath(\"src\")\n",
    "\n",
    "# Si no está ya en sys.path, lo agrega\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"PYTHONPATH actualizado con:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67531cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "import unittest\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "import inspect\n",
    "\n",
    "API_URL = 'http://127.0.0.1:8000/improve'  # Cambia si tu API vive en otra URL\n",
    "BACKUP_DIR = Path('backups')\n",
    "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed1bfa",
   "metadata": {},
   "source": [
    "## Utilidades: respaldo de archivos y helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8a69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_file(src_path: str | Path, backup_root: Path = BACKUP_DIR) -> Path:\n",
    "    \"\"\"Crea un respaldo timestamped del archivo y devuelve la ruta del respaldo.\"\"\"\n",
    "    src_path = Path(src_path)\n",
    "    if not src_path.exists():\n",
    "        raise FileNotFoundError(f\"No existe el archivo: {src_path}\")\n",
    "    ts = datetime.now().strftime('%Y%m%d-%H%M%S-%f')\n",
    "    dst_name = f\"{src_path.stem}__{ts}{src_path.suffix}\"\n",
    "    dst_path = backup_root / dst_name\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    return dst_path\n",
    "\n",
    "def read_text_file(path: str | Path) -> str:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e177ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_from_backup(backup_path: str, original_path: str) -> None:\n",
    "    \"\"\"Restaura un archivo original desde un backup específico.\"\"\"\n",
    "    from pathlib import Path\n",
    "    import shutil\n",
    "    bkp = Path(backup_path)\n",
    "    orig = Path(original_path)\n",
    "    if not bkp.exists():\n",
    "        print(f\"[WARN] Backup no encontrado: {bkp}\")\n",
    "        return\n",
    "    try:\n",
    "        shutil.copy2(bkp, orig)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] No se pudo restaurar {orig} desde {bkp}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373c20b",
   "metadata": {},
   "source": [
    "## Paso 1: Envío a API de mejora y construcción de DataFrame (por iteración)\n",
    "Esta celda sigue tu contrato de API tal como fue provisto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714ee565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_file_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a file path to its corresponding test file path by inserting '_test' \n",
    "    before the file extension.\n",
    "    \n",
    "    Example:\n",
    "    - Input: '/path/to/module.py'\n",
    "    - Output: '/path/to/module_test.py'\n",
    "    \n",
    "    Args:\n",
    "        file_path: The original file path\n",
    "        \n",
    "    Returns:\n",
    "        The path to the corresponding test file\n",
    "    \"\"\"\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "        \n",
    "    # Split the path into directory, filename, and extension\n",
    "    directory, filename = os.path.split(file_path)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    \n",
    "    # Create the test filename by adding '_test' before the extension\n",
    "    test_filename = f\"{name}_test{ext}\"\n",
    "    \n",
    "    # Rejoin the directory with the new test filename\n",
    "    test_file_path = os.path.join(directory, test_filename)\n",
    "    \n",
    "    return test_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580cf34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_one_file(file_path: str, test_module: str, iteration: int, api_url: str = API_URL) -> dict:\n",
    "    \"\"\"\n",
    "    Respaldar, leer y enviar un archivo a la API de mejora.\n",
    "    ⚠️ Ahora REEMPLAZA el archivo original con el `improved_code` retornado por la API.\n",
    "    Devuelve un diccionario listo para ser convertido a fila de DataFrame.\n",
    "    \"\"\"\n",
    "    # 1) Respaldo y lectura\n",
    "    backup_path = backup_file(file_path)\n",
    "    code_content = read_text_file(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    test_content = None\n",
    "\n",
    "    test_path = get_test_file_path(file_path)\n",
    "    if test_module and os.path.exists(test_path):\n",
    "        test_content = read_text_file(test_path)\n",
    "    \n",
    "\n",
    "    # 2) Envío a la API\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    payload = {'Code': code_content}\n",
    "\n",
    "    if test_content:\n",
    "        payload['Tests'] = test_content\n",
    "        \n",
    "    resp = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "    # 3) Procesamiento de la respuesta según tu contrato\n",
    "    if resp.status_code == 200:\n",
    "        data = resp.json()\n",
    "        improved_code = data.get('Code')\n",
    "\n",
    "        # === NUEVO: escribir el código mejorado al archivo original ===\n",
    "        if isinstance(improved_code, str) and improved_code.strip():\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(improved_code)\n",
    "\n",
    "        row = {\n",
    "            'code_file': filename,\n",
    "            'code_file_path': file_path,\n",
    "            'test_file': test_module,\n",
    "            'iteration': iteration,\n",
    "            'backup_path': str(backup_path),\n",
    "            'original_code': code_content,\n",
    "            'improved_code': improved_code,\n",
    "            'analysis': data.get('Analisis'),\n",
    "        }\n",
    "        # Métricas opcionales\n",
    "        metrics = data.get('metrics')\n",
    "        if metrics and isinstance(metrics, dict):\n",
    "            before = metrics.get('before') or {}\n",
    "            after = metrics.get('after') or {}\n",
    "            row.update({\n",
    "                'before_method_number': before.get('method_number'),\n",
    "                'before_ifs': before.get('number_of_ifs'),\n",
    "                'before_loops': before.get('number_of_loops'),\n",
    "                'before_cyclomatic_complexity': before.get('cyclomatic_complexity'),\n",
    "                'before_avg_method_size': before.get('average_method_size'),\n",
    "                'after_method_number': after.get('method_number'),\n",
    "                'after_ifs': after.get('number_of_ifs'),\n",
    "                'after_loops': after.get('number_of_loops'),\n",
    "                'after_cyclomatic_complexity': after.get('cyclomatic_complexity'),\n",
    "                'after_avg_method_size': after.get('average_method_size'),\n",
    "            })\n",
    "        if 'RetrievedContext' in data:\n",
    "            row['retrieved_context'] = json.dumps(data['RetrievedContext'])\n",
    "    else:\n",
    "        row = {\n",
    "            'code_file': filename,\n",
    "            'code_file_path': file_path,\n",
    "            'test_file': test_module,\n",
    "            'iteration': iteration,\n",
    "            'backup_path': str(backup_path),\n",
    "            'error': f\"API Error: {resp.status_code}\",\n",
    "            'error_details': resp.text,\n",
    "        }\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f4a98",
   "metadata": {},
   "source": [
    "## Paso 2: Ejecutar pruebas `unittest` y agregar resultados a las filas (mismo (file, test, iteration))\n",
    "La lógica está basada en tu bloque original, adaptada para conservar la clave `(code_file_path, test_file, iteration)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "206a4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "import importlib.util\n",
    "import os\n",
    "from io import StringIO\n",
    "import contextlib\n",
    "import inspect\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "def _load_module_from_path(path: str):\n",
    "    \"\"\"Load a python module from an absolute or relative file path.\"\"\"\n",
    "    path = os.path.abspath(path)\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"No test file found at: {path}\")\n",
    "    \n",
    "    # Get the directory containing the module\n",
    "    module_dir = os.path.dirname(path)\n",
    "    \n",
    "    # Add the directory to sys.path if it's not already there\n",
    "    if module_dir not in sys.path:\n",
    "        sys.path.insert(0, module_dir)\n",
    "    \n",
    "    # Generate a unique module name\n",
    "    mod_name = f\"__testmod_{uuid.uuid4().hex}\"\n",
    "    \n",
    "    # Create the spec with the package parameter\n",
    "    spec = importlib.util.spec_from_file_location(\n",
    "        mod_name, \n",
    "        path,\n",
    "        # Add the package parameter for handling relative imports\n",
    "        submodule_search_locations=[module_dir]\n",
    "    )\n",
    "    \n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    \n",
    "    # Add the module to sys.modules to handle relative imports\n",
    "    sys.modules[mod_name] = module\n",
    "    \n",
    "    spec.loader.exec_module(module)  # type: ignore[attr-defined]\n",
    "    return module, path\n",
    "\n",
    "\n",
    "def _guess_test_file(candidate: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts diverse inputs:\n",
    "      - a path to a test file (endswith .py) -> return as-is if exists\n",
    "      - a directory path -> try to find *test*.py inside\n",
    "      - a module path (dotted) -> resolve to file via __file__\n",
    "    \"\"\"\n",
    "    # If it's an existing file, return it\n",
    "    p = Path(candidate)\n",
    "    if p.suffix == \".py\" and p.exists():\n",
    "        return str(p)\n",
    "\n",
    "    # If it's a directory, try to find a typical unittest file\n",
    "    if p.exists() and p.is_dir():\n",
    "        # prefer *_test.py then test_*.py\n",
    "        patterns = [\"*test*.py\"]\n",
    "        for pattern in patterns:\n",
    "            matches = sorted(p.glob(pattern))\n",
    "            for m in matches:\n",
    "                return str(m)\n",
    "        raise FileNotFoundError(f\"No test file (*.py) found under directory: {candidate}\")\n",
    "\n",
    "    # If looks like a path without .py, try adding .py\n",
    "    if p.suffix == \"\" and p.exists():\n",
    "        alt = str(p) + \".py\"\n",
    "        if Path(alt).exists():\n",
    "            return alt\n",
    "\n",
    "    # Try to import as module and resolve __file__\n",
    "    try:\n",
    "        mod = importlib.import_module(candidate)\n",
    "        f = getattr(mod, \"__file__\", None)\n",
    "        if f and os.path.isfile(f):\n",
    "            return f\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Finally, if it's something like 'src/dir/name' try appending .py\n",
    "    if p.suffix == \"\" and not p.exists():\n",
    "        alt = candidate + \".py\"\n",
    "        if Path(alt).exists():\n",
    "            return alt\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not resolve a test file from: {candidate}\")\n",
    "\n",
    "def _import_test_target(file_or_module: str):\n",
    "    \"\"\"\n",
    "    Import a test module from either a path (preferred for dirs with hyphens)\n",
    "    or a dotted module path.\n",
    "    \"\"\"\n",
    "    # Try resolve to a concrete file first (supports dirs with hyphens)\n",
    "    try:\n",
    "        file_path = _guess_test_file(file_or_module)\n",
    "        return _load_module_from_path(file_path)\n",
    "    except Exception:\n",
    "        # Fallback: treat as module name (requires importable path without hyphens)\n",
    "        # Also ensure 'src' is in sys.path as a convenience\n",
    "        src_dir = os.path.abspath(\"src\")\n",
    "        if os.path.isdir(src_dir) and src_dir not in sys.path:\n",
    "            sys.path.insert(0, src_dir)\n",
    "        mod = importlib.import_module(file_or_module)\n",
    "        path = getattr(mod, \"__file__\", file_or_module)\n",
    "        return mod, path\n",
    "\n",
    "class _ResultCollector(unittest.TextTestResult):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_results = []  # list[dict]\n",
    "\n",
    "    def addSuccess(self, test):\n",
    "        super().addSuccess(test)\n",
    "        self.test_results.append({\n",
    "            \"test_name\": str(test),\n",
    "            \"status\": \"PASS\",\n",
    "            \"error_message\": None\n",
    "        })\n",
    "\n",
    "    def addFailure(self, test, err):\n",
    "        super().addFailure(test, err)\n",
    "        from traceback import format_exception\n",
    "        msg = \"\".join(format_exception(*err))\n",
    "        self.test_results.append({\n",
    "            \"test_name\": str(test),\n",
    "            \"status\": \"FAIL\",\n",
    "            \"error_message\": msg\n",
    "        })\n",
    "\n",
    "    def addError(self, test, err):\n",
    "        super().addError(test, err)\n",
    "        from traceback import format_exception\n",
    "        msg = \"\".join(format_exception(*err))\n",
    "        self.test_results.append({\n",
    "            \"test_name\": str(test),\n",
    "            \"status\": \"ERROR\",\n",
    "            \"error_message\": msg\n",
    "        })\n",
    "\n",
    "def run_tests_for_file(file_or_module: str, iteration: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a unittest module and run it, returning a DataFrame with per-test results\n",
    "    and a summary row.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_module, resolved_path = _import_test_target(file_or_module)\n",
    "\n",
    "        # Collect TestCase subclasses\n",
    "        test_classes = [\n",
    "            obj for _, obj in inspect.getmembers(test_module)\n",
    "            if inspect.isclass(obj) and issubclass(obj, unittest.TestCase)\n",
    "        ]\n",
    "\n",
    "        # Build a suite from discovered classes\n",
    "        suite = unittest.TestSuite()\n",
    "        loader = unittest.TestLoader()\n",
    "        for cls in test_classes:\n",
    "            suite.addTests(loader.loadTestsFromTestCase(cls))\n",
    "\n",
    "        # Run and capture stdout\n",
    "        stream = StringIO()\n",
    "        runner = unittest.TextTestRunner(stream=stream, verbosity=2, resultclass=_ResultCollector)\n",
    "        start = time.time()\n",
    "        with contextlib.redirect_stdout(stream):\n",
    "            result = runner.run(suite)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        # Build per-test dataframe\n",
    "        rows = []\n",
    "        for r in result.test_results:\n",
    "            print(r)\n",
    "            rows.append({\n",
    "                \"test_file\": resolved_path,\n",
    "                \"iteration\": iteration,\n",
    "                \"test_name\": r[\"test_name\"],\n",
    "                \"status\": r[\"status\"],\n",
    "                \"execution_time\": duration,  # total; fine-grained split omitted\n",
    "                \"error_message\": r[\"error_message\"],\n",
    "            })\n",
    "\n",
    "        total = max(len(rows), 1)\n",
    "        passed = sum(1 for r in rows if r[\"status\"] == \"PASS\")\n",
    "        success_rate = passed / total\n",
    "\n",
    "        summary = {\n",
    "            \"test_file\": resolved_path,\n",
    "            \"iteration\": iteration,\n",
    "            \"tests\": f\"{passed}/{total} ({success_rate:.2%})\",\n",
    "            \"percentage_of_success\": round(success_rate * 100, 2),\n",
    "            \"execution_time\": duration,\n",
    "            \"error\": None,\n",
    "            \"error_details\": None\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(rows) if rows else pd.DataFrame([{\n",
    "            \"test_file\": resolved_path, \"iteration\": iteration,\n",
    "            \"test_name\": \"(no tests found)\", \"status\": \"ERROR\",\n",
    "            \"execution_time\": duration, \"error_message\": \"No unittest.TestCase classes found\"\n",
    "        }])\n",
    "        # Return summary merged with per-test results (like previous notebook)\n",
    "        summary_df = pd.DataFrame([summary])\n",
    "        return df, summary_df\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"{type(e).__name__}: {e}\"\n",
    "        print(err)\n",
    "\n",
    "        print()\n",
    "        summary = pd.DataFrame([{\n",
    "            \"test_file\": file_or_module,\n",
    "            \"iteration\": iteration,\n",
    "            \"tests\": \"ERROR\",\n",
    "            \"percentage_of_success\": 0.0,\n",
    "            \"execution_time\": 0.0,\n",
    "            \"error\": err,\n",
    "            \"error_details\": None\n",
    "        }])\n",
    "        details = pd.DataFrame([{\n",
    "            \"test_file\": file_or_module, \"iteration\": iteration,\n",
    "            \"test_name\": \"(setup failure)\", \"status\": \"ERROR\",\n",
    "            \"execution_time\": 0.0, \"error_message\": err\n",
    "        }])\n",
    "        return details, summary\n",
    "\n",
    "def _run_tests_for_file(file_or_module: str, iteration: int) -> pd.DataFrame:\n",
    "    \"\"\"Compatibility shim to return only the summary dataframe like the original workflow assumed.\"\"\"\n",
    "    _, summary_df = run_tests_for_file(file_or_module, iteration)\n",
    "    return summary_df\n",
    "\n",
    "def run_tests_for_multiple_files(file_paths, iterations: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Run multiple files over several iterations and combine summaries.\"\"\"\n",
    "    all_summaries = []\n",
    "    for file_or_module in file_paths:\n",
    "        for i in range(1, iterations + 1):\n",
    "            _, summary_df = run_tests_for_file(file_or_module, i)\n",
    "            all_summaries.append(summary_df)\n",
    "    return pd.concat(all_summaries, ignore_index=True) if all_summaries else pd.DataFrame(\n",
    "        columns=['test_file', 'iteration', 'tests', 'percentage_of_success', 'execution_time', 'error', 'error_details']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b8e48",
   "metadata": {},
   "source": [
    "## Orquestador: une Paso 1 + Paso 2 en un único DataFrame\n",
    "Se hace un `merge` por `(test_file, iteration)` (y se conserva `code_file_path`) para garantizar que cada envío/iteración tenga los resultados de pruebas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f26f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_workflow(items: list[dict], api_url: str = API_URL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ejecuta el flujo por **iteración**: mejora + escribe, ejecuta tests, MERGE de resultados,\n",
    "    y **restaura el archivo original al final de cada iteración** usando el backup de esa iteración.\n",
    "    \"\"\"\n",
    "    merged_rows = []\n",
    "    for it in items:\n",
    "        file_path = it['file']\n",
    "        test_module = it['test']\n",
    "        iterations = int(it.get('iterations', 1))\n",
    "        for k in range(1, iterations + 1):\n",
    "            # Paso 1 (iteración k): mejora y escritura del código\n",
    "            improve_row = improve_one_file(file_path, test_module, k, api_url)\n",
    "\n",
    "            # Paso 2 (iteración k): correr tests para este test_module/k\n",
    "            tests_df = _run_tests_for_file(test_module, k)\n",
    "\n",
    "            print(\"-\"*10)\n",
    "            print(test_module)\n",
    "            print(tests_df)\n",
    "            print(\"-\"*10)\n",
    "\n",
    "            # Fusionar en una sola fila\n",
    "            import pandas as _pd\n",
    "            improve_df_k = _pd.DataFrame([improve_row])\n",
    "\n",
    "            if 'test_file' not in improve_df_k.columns:\n",
    "                improve_df_k['test_file'] = test_module\n",
    "\n",
    "            merged_k = improve_df_k.merge(\n",
    "                tests_df,\n",
    "                on=['test_file', 'iteration'],\n",
    "                how='left',\n",
    "                suffixes=('', '_test')\n",
    "            )\n",
    "\n",
    "            merged_k['tests'] = tests_df['tests']\n",
    "            merged_k['percentage_of_success'] = tests_df['percentage_of_success']\n",
    "            merged_k['execution_time'] = tests_df['execution_time']\n",
    "            merged_rows.append(merged_k)\n",
    "\n",
    "            # Restaurar inmediatamente al finalizar la iteración\n",
    "            try:\n",
    "                if 'backup_path' in improve_row and improve_row.get('backup_path'):\n",
    "                    restore_from_backup(improve_row['backup_path'], improve_row['code_file_path'])\n",
    "            except Exception as e:\n",
    "                print('[WARN] Falló la restauración por iteración:', e)\n",
    "\n",
    "    # Concatenar todas las filas y ordenar columnas\n",
    "    if not merged_rows:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(merged_rows, ignore_index=True)\n",
    "    preferred_cols = [\n",
    "        'code_file', 'code_file_path', 'backup_path', 'test_file', 'iteration',\n",
    "        'tests', 'percentage_of_success', 'execution_time',\n",
    "        'original_code', 'improved_code', 'analysis', 'retrieved_context',\n",
    "        'before_method_number', 'before_ifs', 'before_loops', 'before_cyclomatic_complexity', 'before_avg_method_size',\n",
    "        'after_method_number', 'after_ifs', 'after_loops', 'after_cyclomatic_complexity', 'after_avg_method_size',\n",
    "        'error', 'error_details'\n",
    "    ]\n",
    "    for col in preferred_cols:\n",
    "        if col not in out.columns:\n",
    "            out[col] = None\n",
    "    return out[preferred_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d450f19",
   "metadata": {},
   "source": [
    "## Ejemplo de uso\n",
    "Ajusta las rutas de `items` a tus archivos locales. Recuerda que `test` **no** lleva `.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a83fc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_name': 'test_hanoi_exception_handling (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_exception_handling)', 'status': 'ERROR', 'error_message': 'Traceback (most recent call last):\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 18, in execute\\n    \"\"\"\\n        \\nTypeError: int() argument must be a string, a bytes-like object or a real number, not \\'object\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 57, in testPartExecutor\\n    yield\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 623, in run\\n    self._callTestMethod(testMethod)\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 579, in _callTestMethod\\n    if method() is not None:\\n       ^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 205, in test_hanoi_exception_handling\\n    result = self.run_hanoi(object())\\n             ^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 93, in run_hanoi\\n    return execute_func(n, src, aux, dst, memo, loud)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 20, in execute\\n    A veces devuelve una lista de movimientos, a veces una cadena, a veces nada.\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: The number of disks must be an integer or convertible to an integer.\\n'}\n",
      "{'test_name': 'test_hanoi_global_state (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_global_state)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_silent_mode (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_silent_mode)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_boolean (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_boolean)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_custom_memo (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_custom_memo)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_custom_pegs (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_custom_pegs)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_empty_list (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_empty_list)', 'status': 'ERROR', 'error_message': 'Traceback (most recent call last):\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 18, in execute\\n    \"\"\"\\n        \\nTypeError: int() argument must be a string, a bytes-like object or a real number, not \\'list\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 57, in testPartExecutor\\n    yield\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 623, in run\\n    self._callTestMethod(testMethod)\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 579, in _callTestMethod\\n    if method() is not None:\\n       ^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 128, in test_hanoi_with_empty_list\\n    result = self.run_hanoi([])\\n             ^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 93, in run_hanoi\\n    return execute_func(n, src, aux, dst, memo, loud)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 20, in execute\\n    A veces devuelve una lista de movimientos, a veces una cadena, a veces nada.\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: The number of disks must be an integer or convertible to an integer.\\n'}\n",
      "{'test_name': 'test_hanoi_with_float_string (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_float_string)', 'status': 'ERROR', 'error_message': 'Traceback (most recent call last):\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 18, in execute\\n    \"\"\"\\n        \\nValueError: invalid literal for int() with base 10: \\'5.0\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 57, in testPartExecutor\\n    yield\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 623, in run\\n    self._callTestMethod(testMethod)\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 579, in _callTestMethod\\n    if method() is not None:\\n       ^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 113, in test_hanoi_with_float_string\\n    result = self.run_hanoi(\"5.0\")\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 93, in run_hanoi\\n    return execute_func(n, src, aux, dst, memo, loud)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 20, in execute\\n    A veces devuelve una lista de movimientos, a veces una cadena, a veces nada.\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: The number of disks must be an integer or convertible to an integer.\\n'}\n",
      "{'test_name': 'test_hanoi_with_integer (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_integer)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_list (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_list)', 'status': 'ERROR', 'error_message': 'Traceback (most recent call last):\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 18, in execute\\n    \"\"\"\\n        \\nTypeError: int() argument must be a string, a bytes-like object or a real number, not \\'list\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 57, in testPartExecutor\\n    yield\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 623, in run\\n    self._callTestMethod(testMethod)\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 579, in _callTestMethod\\n    if method() is not None:\\n       ^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 121, in test_hanoi_with_list\\n    result = self.run_hanoi(test_list)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 93, in run_hanoi\\n    return execute_func(n, src, aux, dst, memo, loud)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 20, in execute\\n    A veces devuelve una lista de movimientos, a veces una cadena, a veces nada.\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: The number of disks must be an integer or convertible to an integer.\\n'}\n",
      "{'test_name': 'test_hanoi_with_negative (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_negative)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_non_string_pegs (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_non_string_pegs)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_none (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_none)', 'status': 'ERROR', 'error_message': 'Traceback (most recent call last):\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 18, in execute\\n    \"\"\"\\n        \\nTypeError: int() argument must be a string, a bytes-like object or a real number, not \\'NoneType\\'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 57, in testPartExecutor\\n    yield\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 623, in run\\n    self._callTestMethod(testMethod)\\n  File \"/Users/slincastro/.pyenv/versions/3.11.7/lib/python3.11/unittest/case.py\", line 579, in _callTestMethod\\n    if method() is not None:\\n       ^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 144, in test_hanoi_with_none\\n    result = self.run_hanoi(None)\\n             ^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers_test.py\", line 93, in run_hanoi\\n    return execute_func(n, src, aux, dst, memo, loud)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/Users/slincastro/Projects/Master_IA/final_project/saucode/evals/src/exercise4_hanoi/hanoi_towers.py\", line 20, in execute\\n    A veces devuelve una lista de movimientos, a veces una cadena, a veces nada.\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nValueError: The number of disks must be an integer or convertible to an integer.\\n'}\n",
      "{'test_name': 'test_hanoi_with_string (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_string)', 'status': 'PASS', 'error_message': None}\n",
      "{'test_name': 'test_hanoi_with_zero (__testmod_c6cb0d494a254b4baf61491e56b79186.TestHanoiTowers.test_hanoi_with_zero)', 'status': 'PASS', 'error_message': None}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_file</th>\n",
       "      <th>iteration</th>\n",
       "      <th>tests</th>\n",
       "      <th>percentage_of_success</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>error</th>\n",
       "      <th>error_details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/slincastro/Projects/Master_IA/final_pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>10/15 (66.67%)</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.008786</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           test_file  iteration  \\\n",
       "0  /Users/slincastro/Projects/Master_IA/final_pro...          1   \n",
       "\n",
       "            tests  percentage_of_success  execution_time error error_details  \n",
       "0  10/15 (66.67%)                  66.67        0.008786  None          None  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_run_tests_for_file('./src/exercise4_hanoi/hanoi_towers_test.py',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "238ee28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError: the 'package' argument is required to perform a relative import for './src/exercise4_hanoi/hanoi_towers_test.py'\n",
      "\n",
      "----------\n",
      "./src/exercise4_hanoi/hanoi_towers_test.py\n",
      "                                    test_file  iteration  tests  \\\n",
      "0  ./src/exercise4_hanoi/hanoi_towers_test.py          1  ERROR   \n",
      "\n",
      "   percentage_of_success  execution_time  \\\n",
      "0                    0.0             0.0   \n",
      "\n",
      "                                               error error_details  \n",
      "0  TypeError: the 'package' argument is required ...          None  \n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_file</th>\n",
       "      <th>code_file_path</th>\n",
       "      <th>backup_path</th>\n",
       "      <th>test_file</th>\n",
       "      <th>iteration</th>\n",
       "      <th>tests</th>\n",
       "      <th>percentage_of_success</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>original_code</th>\n",
       "      <th>improved_code</th>\n",
       "      <th>...</th>\n",
       "      <th>before_loops</th>\n",
       "      <th>before_cyclomatic_complexity</th>\n",
       "      <th>before_avg_method_size</th>\n",
       "      <th>after_method_number</th>\n",
       "      <th>after_ifs</th>\n",
       "      <th>after_loops</th>\n",
       "      <th>after_cyclomatic_complexity</th>\n",
       "      <th>after_avg_method_size</th>\n",
       "      <th>error</th>\n",
       "      <th>error_details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hanoi_towers.py</td>\n",
       "      <td>src/exercise4_hanoi/hanoi_towers.py</td>\n",
       "      <td>backups/hanoi_towers__20251020-164448-492121.py</td>\n",
       "      <td>./src/exercise4_hanoi/hanoi_towers_test.py</td>\n",
       "      <td>1</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td># WARNING: Código intencionalmente horrible pa...</td>\n",
       "      <td>def execute(n, src=\"A\", aux=\"B\", dst=\"C\", memo...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>18.5</td>\n",
       "      <td>TypeError: the 'package' argument is required ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         code_file                       code_file_path  \\\n",
       "0  hanoi_towers.py  src/exercise4_hanoi/hanoi_towers.py   \n",
       "\n",
       "                                       backup_path  \\\n",
       "0  backups/hanoi_towers__20251020-164448-492121.py   \n",
       "\n",
       "                                    test_file  iteration  tests  \\\n",
       "0  ./src/exercise4_hanoi/hanoi_towers_test.py          1  ERROR   \n",
       "\n",
       "   percentage_of_success  execution_time  \\\n",
       "0                    0.0             0.0   \n",
       "\n",
       "                                       original_code  \\\n",
       "0  # WARNING: Código intencionalmente horrible pa...   \n",
       "\n",
       "                                       improved_code  ... before_loops  \\\n",
       "0  def execute(n, src=\"A\", aux=\"B\", dst=\"C\", memo...  ...            1   \n",
       "\n",
       "  before_cyclomatic_complexity  before_avg_method_size  after_method_number  \\\n",
       "0                           22                    72.0                    2   \n",
       "\n",
       "   after_ifs  after_loops  after_cyclomatic_complexity  after_avg_method_size  \\\n",
       "0          4            0                            6                   18.5   \n",
       "\n",
       "                                               error  error_details  \n",
       "0  TypeError: the 'package' argument is required ...           None  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "items = [\n",
    "#    {\n",
    "#        'file': 'src/exercise1_fibonacci/fibonacci.py',\n",
    "#        'test': './src/exercise1_fibonacci/fibonacci_test.py',\n",
    "#        'iterations': 3\n",
    "#    },\n",
    "#    {\n",
    "#        'file': 'src/exercise2_factorial/factorial.py',\n",
    "#        'test': './src/exercise2_factorial/factorial_test.py',\n",
    "#        'iterations': 1\n",
    "#   }\n",
    "# ,\n",
    "#    {\n",
    "#        'file': 'src/exercise3_calculate_pi/calculate_pi.py',\n",
    "#        'test': './src/exercise3_calculate_pi/calculate_pi_test.py',\n",
    "#        'iterations': 3\n",
    "#    }\n",
    "    {\n",
    "        'file': 'src/exercise4_hanoi/hanoi_towers.py',\n",
    "        'test': './src/exercise4_hanoi/hanoi_towers_test.py',\n",
    "        'iterations': 1\n",
    "   }\n",
    "]\n",
    "\n",
    "try:\n",
    "    df = run_full_workflow(items, api_url=API_URL)\n",
    "    display(df)\n",
    "except Exception as e:\n",
    "    print('[WARN] No se pudo ejecutar el flujo completo aquí (por ejemplo, si la API no está disponible).')\n",
    "    print('Error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f388660",
   "metadata": {},
   "source": [
    "## Guardar resultados (opcional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8845607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta para guardar a CSV\n",
    "# if 'df' in globals() and isinstance(df, pd.DataFrame):\n",
    "#     df.to_csv('improvement_and_tests_results.csv', index=False)\n",
    "#     print('Resultados guardados en improvement_and_tests_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
